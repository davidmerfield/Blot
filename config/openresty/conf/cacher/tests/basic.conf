user {{user}} {{group}};

events {
    worker_connections 1000;
    multi_accept on;
}

http {

    log_format access_log_format '[$time_local] ID=$request_id STATUS=$status REQ_TIME=$request_time REQ_LEN=$request_length RES_LEN=$bytes_sent $scheme://$host$request_uri CACHE_STATUS=$sent_http_cache_status';

    error_log {{{data_directory}}}/error.log info;
    access_log {{{data_directory}}}/access.log access_log_format;

    proxy_cache_path {{{cache_directory}}} levels=1:2 keys_zone=PROXY_CACHE:10m inactive=1y max_size=10g use_temp_path=off;

    lua_shared_dict cacher_dictionary 50m;
    lua_package_path "{{{lua_directory}}}/?.lua;;";

    init_by_lua_block {
        cacher = (require "cacher").new()
        cacher:set("cache_directory", "{{{cache_directory}}}")
        cacher:set("shared_dictionary", ngx.shared.cacher_dictionary)
        cacher:rehydrate(ngx)
    }

    server {
        listen {{openresty.port}} default_server;

        location = /purge {
            content_by_lua_block {
                cacher:purge(ngx)
            }
        }

        location = /health {
            return 200 'Hello, World!';
        }

        location / {
            open_file_cache off;

            proxy_cache            PROXY_CACHE;
            proxy_cache_valid      200 301 302 304 1y;
            proxy_cache_key $scheme://$host:$server_port$request_uri;
            proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504;

            add_header 'Cache-Status' $upstream_cache_status always;

            proxy_pass http://127.0.0.1:{{node.port}};
            proxy_http_version 1.1;

            # This registers a cached response in our dictionary of cached responses
            log_by_lua_block {
                cacher:add(ngx)
            }
        }
    }
}