# Downtime checklist

Questions to answer:

Can you SSH into the instance?
- if not, try 'stopping' then 'starting' the instance through AWS, which is different to a system 'reboot' and may provision new hardware for the instance

Can you OS-level reboot the instance?
- if not, try 'stopping' then 'starting' the instance through AWS, which is different to a system 'reboot' and may provision new hardware for the instance

Does the instance have free disk space?
- if not, trying removing log files

Does the instance have free memory?
- if not, try reseting blogs' search index

What do you see in the logs?
- resolve any obvious issues

Check AWS console cloudwatch metrics:
- Do you see inbound network packets (is this a networking issue?) on AWS cloudwatch metrics
- Do you see elevated or missing CPU usage information on AWS cloudwatch metrics for the instance
- Are there obvious patterns to these metrics?

Good commands:
sudo grep -i -r 'out of memory' /var/log/


When the server is unreponsive:
- is redis bgsave the culprit? unlikely
- it seems that based on this:
  sudo grep -i -r 'out of memory' /var/log/
  pandoc is being killed around some of the downtimes
  let's investigate the pandoc memory limits:
  https://pandoc.org/MANUAL.html#a-note-on-security
- remediations:
  - begin dumping output of 'top' when memuse > 90%?
- server became unresponsive to ssh at multiple incidents
- Work out if RPS was elevated?
- To find notifications: 
  https://mail.google.com/mail/u/0/#search/subject%3A%22%E2%96%BC+%5Bupdown+alert%5D%5BDOWN%5D+Blot%22
- Incidents: 
  - 27 Jul 22:10 UTC 
      node: cat logs/archive-2021-07-28-ec2-user/app.log | grep "22\:10\:33"
      redis: cat logs/archive-2021-07-28-ec2-user/redis.log | grep "27 Jul 22\:0" -A 20 -B 20
      monit: cat logs/archive-2021-07-28-ec2-user/monit.log | grep "22\:10\:" -B 10 -A 10
      system: cat logs/archive-2021-07-28-ec2-user/sys.log | grep "T22\:" -B 100
      nginx: cat logs/archive-2021-07-28-ec2-user/nginx.log | grep "22\:10\:35" -B 100
      nginx error: cat logs/archive-2021-07-28-root/nginx.error.log | grep "22\:10\:36" -B 15 -A 5
      looking at app.log, memuse spikes from 60 to 90% a few minutes before the issue:
      cat logs/archive-2021-07-28-ec2-user/app.log | grep "22\:10\:33" -B 10000 | grep memuse
        big sync blog_2be3cf8 sync_0b963ae from 22:06:00 -> 22:06:11
        nothing too out of the ordinary though
  - Jul 27, 21:40 UTC
  - Jul 27, 07:03 UTC
  - Jul 27, 06:30 UTC
  - Jul 27, 06:03 UTC
  - Jul 27, 05:53 UTC
      node: cat logs/archive-2021-07-28-ec2-user/app.log | grep "05\:53\:28" -A 10


Find posts that began to built, but never succeeded:
  cat logs/app.log | grep "a83f9b303c72f6c0e86f4bee8a0c2e13" -B 10000 | grep "Saving file in database succeeded$" | cut -d " " -f5- | sort | uniq | rev | cut -d " " -f 6- | rev | sort > tmp/paths-succeeded.txt

  cat logs/app.log | grep "a83f9b303c72f6c0e86f4bee8a0c2e13" -B 10000 | grep "Saving file in database$" | cut -d " " -f5- | sort | uniq | rev | cut -d " " -f 5- | rev | sort  > tmp/paths-started.txt

  comm tmp/paths-started.txt tmp/paths-succeeded.txt -2 -3

  This did uncover a post whose sync process began but did not end, shortly before the downtime. I tried to rebuild this post, after applying the time and memory limits to the pandoc process, and it did indeed break those limits.

a83f9b303c72f6c0e86f4bee8a0c2e13
  
fc114dc8cc0b897c221b168ee96fa0a2:
  /drafts/[name]/a23/2 feb/calibre/unknown/b071yx3db5 ebok (17)/b071yx3db5 ebok - unknown.md  
  