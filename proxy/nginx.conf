

# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# !!!!!!!!!!!   WARNING                                   !!!!!!!!!!!
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

# Do not edit this file directly

# This file was generated by ../build.js
# Please update the source files in ./conf and run ../build.js

# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# !!!!!!!!!!!   WARNING                                   !!!!!!!!!!!
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

user	  ec2-user ec2-user;  ## Default: nobody

worker_processes 2;

# Defines the scheduling priority for worker processes like it is done by the nice command:
# a negative number means higher priority. Allowed range normally varies from -20 to 20.
worker_priority -20;

# Sets the limit of the maximum number of open files (RLIMIT_NOFILE) 
# for worker processes
worker_rlimit_nofile 10000;

events {
    worker_connections 10000;
    multi_accept on;
}
http {

    log_format access_log_format '[$time_local] $request_id $status $request_time $request_length:$bytes_sent $scheme://$host$request_uri  cache=$sent_http_blot_cache';

    error_log /var/instance-ssd/logs/error.log info;
    access_log /var/instance-ssd/logs/access.log access_log_format;

    # Hide the nginx version in the server header
    server_tokens off;

    # Added to set the content-type charset header 
    # for text files served by NGINX
    charset utf-8;

    # Taken from 
    # https://github.com/h5bp/server-configs-nginx/blob/master/mime.types

    types {

      # Data interchange

        application/atom+xml                  atom;
        application/json                      json map topojson;
        application/ld+json                   jsonld;
        application/rss+xml                   rss;
        application/vnd.geo+json              geojson;
        application/xml                       rdf xml;


      # JavaScript

        # Normalize to standard type.
        # https://tools.ietf.org/html/rfc4329#section-7.2
        application/javascript                js jsgzip;


      # Manifest files

        application/manifest+json             webmanifest;
        application/x-web-app-manifest+json   webapp;
        text/cache-manifest                   appcache;


      # Media files

        audio/midi                            mid midi kar;
        audio/mp4                             aac f4a f4b m4a;
        audio/mpeg                            mp3;
        audio/ogg                             oga ogg opus;
        audio/x-realaudio                     ra;
        audio/x-wav                           wav;
        image/bmp                             bmp;
        image/gif                             gif;
        image/jpeg                            jpeg jpg;
        image/jxr                             jxr hdp wdp;
        image/png                             png;
        image/svg+xml                         svg svggzip;
        image/tiff                            tif tiff;
        image/vnd.wap.wbmp                    wbmp;
        image/webp                            webp;
        image/x-jng                           jng;
        video/3gpp                            3gp 3gpp;
        video/mp4                             f4p f4v m4v mp4;
        video/mpeg                            mpeg mpg;
        video/ogg                             ogv;
        video/quicktime                       mov;
        video/webm                            webm;
        video/x-flv                           flv;
        video/x-mng                           mng;
        video/x-ms-asf                        asf asx;
        video/x-ms-wmv                        wmv;
        video/x-msvideo                       avi;

        # Serving `.ico` image files with a different media type
        # prevents Internet Explorer from displaying then as images:
        # https://github.com/h5bp/html5-boilerplate/commit/37b5fec090d00f38de64b591bcddcb205aadf8ee

        image/x-icon                          cur ico;


      # Microsoft Office

        application/msword                                                         doc;
        application/vnd.ms-excel                                                   xls;
        application/vnd.ms-powerpoint                                              ppt;
        application/vnd.openxmlformats-officedocument.wordprocessingml.document    docx;
        application/vnd.openxmlformats-officedocument.spreadsheetml.sheet          xlsx;
        application/vnd.openxmlformats-officedocument.presentationml.presentation  pptx;


      # Web fonts

        application/font-woff                 woff;
        application/font-woff2                woff2;
        application/vnd.ms-fontobject         eot;

        # Browsers usually ignore the font media types and simply sniff
        # the bytes to figure out the font type.
        # https://mimesniff.spec.whatwg.org/#matching-a-font-type-pattern
        #
        # However, Blink and WebKit based browsers will show a warning
        # in the console if the following font types are served with any
        # other media types.

        application/x-font-ttf                ttc ttf;
        font/opentype                         otf;


      # Other

        application/java-archive              ear jar war;
        application/mac-binhex40              hqx;
        application/octet-stream              bin deb dll dmg exe img iso msi msm msp safariextz;
        application/pdf                       pdf;
        application/postscript                ai eps ps;
        application/rtf                       rtf;
        application/vnd.google-earth.kml+xml  kml;
        application/vnd.google-earth.kmz      kmz;
        application/vnd.wap.wmlc              wmlc;
        application/x-7z-compressed           7z;
        application/x-bb-appworld             bbaw;
        application/x-bittorrent              torrent;
        application/x-chrome-extension        crx;
        application/x-cocoa                   cco;
        application/x-java-archive-diff       jardiff;
        application/x-java-jnlp-file          jnlp;
        application/x-makeself                run;
        application/x-opera-extension         oex;
        application/x-perl                    pl pm;
        application/x-pilot                   pdb prc;
        application/x-rar-compressed          rar;
        application/x-redhat-package-manager  rpm;
        application/x-sea                     sea;
        application/x-shockwave-flash         swf;
        application/x-stuffit                 sit;
        application/x-tcl                     tcl tk;
        application/x-x509-ca-cert            crt der pem;
        application/x-xpinstall               xpi;
        application/xhtml+xml                 xhtml;
        application/xslt+xml                  xsl;
        application/zip                       zip;
        text/css                              css cssgzip;
        text/csv                              csv;
        text/html                             htm html shtml htmlgzip;
        text/markdown                         md;
        text/mathml                           mml;
        text/plain                            txt;
        text/vcard                            vcard vcf;
        text/vnd.rim.location.xloc            xloc;
        text/vnd.sun.j2me.app-descriptor      jad;
        text/vnd.wap.wml                      wml;
        text/vtt                              vtt;
        text/x-component                      htc;

    }

    open_file_cache          max=1000 inactive=3600s;
    open_file_cache_valid    60s;
    open_file_cache_min_uses 1;
    open_file_cache_errors   on;

    # do we get better performance if we disable this feature
    # rather than resending the entire file?
    # if_modified_since off;

    sendfile on;
    sendfile_max_chunk 1m;

    tcp_nopush on;
    tcp_nodelay on;

    gzip on;
    gzip_types text/plain text/xml text/css
               text/comma-separated-values
               application/javascript
               text/javascript application/x-javascript
               application/atom+xml;

    lua_package_path '/home/ec2-user/openresty/?.lua;;';

    # The "auto_ssl" shared dict should be defined with enough storage space to
    # hold your certificate data. 1MB of storage holds certificates for
    # approximately 100 separate domains. Note that this should not cause an
    # error if there are too many domains, just that nginx will have to look
    # up the certificate in the database which is slower.
    # 100mb = 10,000 domains
    lua_shared_dict auto_ssl 100m;

    # The "auto_ssl" shared dict is used to temporarily store various settings
    # like the secret used by the hook server on port 8999. Do not change or omit.
    lua_shared_dict auto_ssl_settings 64k;

    # A DNS resolver must be defined for OCSP stapling to function.
    #
    # This example uses Google's DNS server. You may want to use your system's
    # default DNS servers, which can be found in /etc/resolv.conf. If your network
    # is not IPv6 compatible, you may wish to disable IPv6 results by using the
    # "ipv6=off" flag (like "resolver 8.8.8.8 ipv6=off").
    # https://github.com/auto-ssl/lua-resty-auto-ssl/issues/12#issuecomment-259402817
    # found by running cat /etc/resolv.conf and looking for the nameserver value
    resolver 8.8.8.8 ipv6=off;

    # Internal server running on port 8999 for handling certificate tasks.
    server {

      listen 127.0.0.1:8999;

      # Increase the body buffer size, to ensure the internal POSTs can always
      # parse the full POST contents into memory.
      client_body_buffer_size 128k;
      client_max_body_size 128k;

      location / {
        content_by_lua_block {
          auto_ssl:hook_server()
        }
      }
    }

    init_by_lua_block {

        cacher = (require "cacher").new()
        cacher:set("cache_directory", "/var/instance-ssd/cache")
        cacher:set("shared_dictionary", ngx.shared.cacher_dictionary)
        cacher:set("minimum_free_space", 1024 * 1024 * 2) 

        auto_ssl = (require "resty.auto-ssl").new()

        local redis = require "resty.redis"

        local redis_options = { host = "172.30.0.138", port = 6379 , prefix = "ssl" }

        local function get_redis_instance(redis_options)

            local instance = ngx.ctx.auto_ssl_redis_instance

            if instance then
                return instance
            end

            instance = redis:new()

            local ok, err

            if redis_options["socket"] then
                ok, err = instance:connect(redis_options["socket"])
            else
                ok, err = instance:connect(redis_options["host"], redis_options["port"])
            end

            if not ok then
                return false, err
            end

            if redis_options["auth"] then
                ok, err = instance:auth(redis_options["auth"])
                if not ok then
                return false, err
                end
            end

            ngx.ctx.auto_ssl_redis_instance = instance
            return instance
        end

        auto_ssl:set("redis", redis_options)

        -- Certificates are stored in redis
        auto_ssl:set("storage_adapter", "resty.auto-ssl.storage_adapters.redis")

        -- This function determines whether the incoming domain
        -- should automatically issue a new SSL certificate.
        -- I need to set domain:blot.im to foo in the database so that
        -- the allow_domain function works as expected even though
        -- it's not technically a user's domain

        local function allow_domain(domain)

            local certstorage = auto_ssl.storage
            
            local fullchain_pem, privkey_pem = certstorage:get_cert(domain)

            -- If we have this cert in the memory cache
            -- then return it without checking redis to save time
            if fullchain_pem then
                return true
            end

            local redis_instance, instance_err = get_redis_instance(redis_options)

            if instance_err then
                return nil, instance_err
            end
        
            local res, err = redis_instance:get('domain:' .. domain)

            if res == ngx.null then
                return false
            end

            return true
        end

        auto_ssl:set("allow_domain", allow_domain)

        auto_ssl:init()
    }

    init_worker_by_lua_block {
      auto_ssl:init_worker()
      if ngx.worker.id() == 0 then
        cacher:monitor_free_space(ngx)
      end
    }
    # Make sure this directory exists
    # It does not need to be owned by root
    # https://serverfault.com/questions/1029358/nginx-keys-zone-size-persistence-and-maximum-number-of-files
    # 1m = ~8000 items
    # I believe the nginx memory cache does not need to be as large as cacher_dictionary because it can fall back 
    # to disk in an elegant way
    proxy_cache_path  /var/instance-ssd/cache  levels=1:2 keys_zone=PROXY_CACHE:30m inactive=1y max_size=200g use_temp_path=off;

    lua_shared_dict cacher_dictionary 75m;

    upstream blot_node {
      # Use least connections algorithm to better distribute load
      least_conn;
      # Activates the cache for connections to upstream servers.
      keepalive 120;

      # We use the default fail_timeout=10s and max_fails=1 since
      # there is a backup server which should never be marked as
      # unavailable using the max_fails=0 directive
      server 127.0.0.1:8089;                    # blot-container-green MASTER server
      server 127.0.0.1:8088 backup max_fails=0; # blot-container-blue FAILOVER server
    }

    upstream blot_blogs_node {
      # Use least connections algorithm to better distribute load
      least_conn;
      # Activates the cache for connections to upstream servers.
      keepalive 120;

      # We use the default fail_timeout=10s and max_fails=1 since
      # there is a backup server which is never marked as unavailable
      # We bias traffic to one container slightly so if there is a memory
      # leak both containers do not go down close together
      server 127.0.0.1:8090 weight=4;           # blot-container-yellow
      server 127.0.0.1:8091 weight=3;           # blot-container-purple
      server 127.0.0.1:8088 backup max_fails=0; # blot-container-blue FAILOVER server
    }


    upstream blot_stats {
      server 127.0.0.1:19999;
    }    
    # blot subdomains (e.g. david.blot.im)
    server {
        listen 80;
        listen 443 ssl;
        
        http2 on;

        # match all subdomains of blot.im which do not start with preview-
        # e.g. blog-on-david.blot.im
        server_name "~^(?!preview-)[^.]+\.blot.im$";
        
        # Check with the SSL evaluator tool
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_prefer_server_ciphers off;
        ssl_ciphers "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384";

        # Hopefully improves performance
        # one megabyte can store about 4000 sessions
        # 10m = 40,000 sessions
        ssl_session_cache shared:SSL:10m;
        ssl_session_timeout 1h;
        ssl_session_tickets off;

        # Add OCSP stapling if your cert supports it
        ssl_stapling on;
        ssl_stapling_verify on;

        # The SSL certificate we use is generated by https://github.com/kshcherban/acme-nginx
        # for the "*.blot.im" and "blot.im" domains. This allows us to do an infinite
        # number of subdomains for previewing templates. The paths to these files are
        # hardcoded in the source for kshcherban/acme-nginx. I wrote more about this
        # process in notes/wildcard-ssl
        ssl_certificate /etc/ssl/private/letsencrypt-domain.pem;
        ssl_certificate_key /etc/ssl/private/letsencrypt-domain.key;

        set $upstream_server blot_blogs_node;
        root /;

        # Match urls which start with /draft/stream/
        location /draft/stream/ {
          proxy_pass http://blot_node;
          proxy_read_timeout 24h;
          proxy_send_timeout 24h;
          proxy_connect_timeout 24h;

          # SSE-specific configurations
          proxy_http_version 1.1;      # SSE requires HTTP/1.1
          proxy_set_header Connection '';  # Disable keep-alive
          proxy_buffering off;         # Disable response buffering
          proxy_cache off;             # Ensure no caching

          # generic
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
        }

        # This is used to determine whether the server is handling
        # requests, don't remove it unless you change monit.rc too!
        location = /health {
          return 200;
        }

        # This is used to prevent people from accessing the git repositories in user folders
        location ^~ /.git {
          return 404;
        }

        # return 404 immediately for all requests to URLs ending in .php, .asp, .aspx, .jsp, .php5, .php7, .php8
        location ~* \.(?:php[0-9]?|asp|aspx|jsp)$ {
          return 404;
        }

        # return 404 immediate for /.vscode/sftp.json
        location = /.vscode/sftp.json {
          return 404;
        }

        location = /sftp-config.json {
          return 404;
        }

        location = /magento_version {
          return 404;
        }

        # return 404 immediately for all requests to URLs starting with /api/v1
        location ~* ^/api/v1 {
          return 404;
        }

        location ~* ^/api/collections/ {
          return 404;
        }

        location ~* ^/vendor/phpunit/ {
          return 404;
        }

        # return 404 for all URLS starting with /admin/controller/extension/extension
        location ~* ^/admin/controller/extension/extension {
          return 404;
        }

        # return 404 immediately for all requests to URLs starting with /admin/.git/
        location ~* ^/admin/.git {
          return 404;
        }

        # return 404 immediately for all requests containing 'index.php/' or 'admin.php/' anywhere in their path
        location ~* (?:index|admin)\.php/ {
          return 404;
        }

        # return 404 immediately for all requests to URLs ending in .env
        location ~* \.env$ {
          return 404;
        }

        # return 404 immediately for all requests to URLs containing '/wp-admin/', '/wp-content/', '/wp-includes/' anywhere in their path
        location ~* wp-(?:admin|content|includes|diambar|json|config) {
          return 404;
        }

        # bypass the cache for requests to /random
        # todo: handle this with headers instead of a separate location block
        location = /random {
          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream
        }

        # error page for when node server is unavailable
        location = /error-upstream-offline.html {
          root  /home/ec2-user/openresty/html;
          try_files /error-upstream-offline-blog.html =404;
        }

        location / {
          set $cache_key $scheme://$host$request_uri;

          open_file_cache off;

          proxy_cache            PROXY_CACHE;

          # https://cassiomolin.com/2016/09/09/which-http-status-codes-are-cacheable/
          proxy_cache_valid  200 301 302 400 404  1y;

          # we want to cache everything, even if cache-control says not to
          # it's important to ignore the vary header from upstream otherwise
          # nginx will cache multiple copies of the same page
          proxy_ignore_headers Cache-Control Expires Set-Cookie Vary;

          proxy_cache_key $cache_key;
          proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504;

          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream

          # This registers a cached response in our dictionary of cached responses
          # pass in the proxy_cache_key setting
          log_by_lua_block {
              if ngx.var.upstream_cache_status == "MISS" then
                  cacher:add(ngx.var.host, ngx.var.cache_key)
              end
          }        }

    }

    # preview subdomains (e.g. preview-of-blog-on-david.blot.im)
    # these skip the cache and are passed directly to node
    server {
        listen 80;
        listen 443 ssl;
        
        http2 on;

        server_name "~^preview-[^.]+\.blot.im$";

        # Check with the SSL evaluator tool
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_prefer_server_ciphers off;
        ssl_ciphers "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384";

        # Hopefully improves performance
        # one megabyte can store about 4000 sessions
        # 10m = 40,000 sessions
        ssl_session_cache shared:SSL:10m;
        ssl_session_timeout 1h;
        ssl_session_tickets off;

        # Add OCSP stapling if your cert supports it
        ssl_stapling on;
        ssl_stapling_verify on;

        # The SSL certificate we use is generated by https://github.com/kshcherban/acme-nginx
        # for the "*.blot.im" and "blot.im" domains. This allows us to do an infinite
        # number of subdomains for previewing templates. The paths to these files are
        # hardcoded in the source for kshcherban/acme-nginx. I wrote more about this
        # process in notes/wildcard-ssl
        ssl_certificate /etc/ssl/private/letsencrypt-domain.pem;
        ssl_certificate_key /etc/ssl/private/letsencrypt-domain.key;        
        location / {
            set $upstream_server blot_blogs_node;
            add_header 'Blot-Server' 'us' always;
            add_header 'Blot-Cache' $upstream_cache_status always;
            add_header 'Blot-Upstream' $upstream_addr always;

            proxy_pass http://$upstream_server;
            proxy_http_version 1.1;

            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header X-Request-ID $request_id;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
            # We let nginx handle the compression
            proxy_set_header Accept-Encoding "";

            gzip on;
            gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
            gzip_comp_level 6;
            gzip_proxied any;

            # For larger responses (blog posts, content-heavy pages)
            # Configured for 2MB total buffer size
            proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
            proxy_buffer_size 64k;

            proxy_redirect off;

            proxy_intercept_errors on;
            error_page 500 502 503 504 429 /error-upstream-offline.html;

            # Enable retrying the next upstream on failure
            # Important to limit the number of tries and possibly
            # the timeout otherwise a single request which triggers an error could take down all the servers
            # We do rolling deploys where one server is taken down at a time so this should allow 
            # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
            # You have to be careful that these error codes are only returned when the server is truly
            # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
            # a 500 error code, and this config considered the upstream server bad!
            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
            proxy_next_upstream_tries 1;
            proxy_next_upstream_timeout 10s;

            # Fine-tune timeouts for better failover
            proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
            proxy_send_timeout 10s;    # Time to send the request to the upstream
            proxy_read_timeout 15s;    # Time to read the response from the upstream
        }
    }

    # cdn subdomain (e.g. cdn.blot.im)
    # these skip the cache and are passed directly to node
    server {
        listen 80;
        listen 443 ssl;

        http2 on;

        server_name cdn.blot.im;

        # Check with the SSL evaluator tool
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_prefer_server_ciphers off;
        ssl_ciphers "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384";

        # Hopefully improves performance
        # one megabyte can store about 4000 sessions
        # 10m = 40,000 sessions
        ssl_session_cache shared:SSL:10m;
        ssl_session_timeout 1h;
        ssl_session_tickets off;

        # Add OCSP stapling if your cert supports it
        ssl_stapling on;
        ssl_stapling_verify on;

        # The SSL certificate we use is generated by https://github.com/kshcherban/acme-nginx
        # for the "*.blot.im" and "blot.im" domains. This allows us to do an infinite
        # number of subdomains for previewing templates. The paths to these files are
        # hardcoded in the source for kshcherban/acme-nginx. I wrote more about this
        # process in notes/wildcard-ssl
        ssl_certificate /etc/ssl/private/letsencrypt-domain.pem;
        ssl_certificate_key /etc/ssl/private/letsencrypt-domain.key;        
        location / {
            set $upstream_server blot_blogs_node;
            add_header 'Blot-Server' 'us' always;
            add_header 'Blot-Cache' $upstream_cache_status always;
            add_header 'Blot-Upstream' $upstream_addr always;

            proxy_pass http://$upstream_server;
            proxy_http_version 1.1;

            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header X-Request-ID $request_id;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
            # We let nginx handle the compression
            proxy_set_header Accept-Encoding "";

            gzip on;
            gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
            gzip_comp_level 6;
            gzip_proxied any;

            # For larger responses (blog posts, content-heavy pages)
            # Configured for 2MB total buffer size
            proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
            proxy_buffer_size 64k;

            proxy_redirect off;

            proxy_intercept_errors on;
            error_page 500 502 503 504 429 /error-upstream-offline.html;

            # Enable retrying the next upstream on failure
            # Important to limit the number of tries and possibly
            # the timeout otherwise a single request which triggers an error could take down all the servers
            # We do rolling deploys where one server is taken down at a time so this should allow 
            # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
            # You have to be careful that these error codes are only returned when the server is truly
            # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
            # a 500 error code, and this config considered the upstream server bad!
            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
            proxy_next_upstream_tries 1;
            proxy_next_upstream_timeout 10s;

            # Fine-tune timeouts for better failover
            proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
            proxy_send_timeout 10s;    # Time to send the request to the upstream
            proxy_read_timeout 15s;    # Time to read the response from the upstream
        }
    }

    # stats subdomain 
    # these skip the cache and are passed directly to node
    server {
        listen 80;
        listen 443 ssl;
        http2 on;

        server_name stats.blot.im;

        # Check with the SSL evaluator tool
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_prefer_server_ciphers off;
        ssl_ciphers "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384";

        # Hopefully improves performance
        # one megabyte can store about 4000 sessions
        # 10m = 40,000 sessions
        ssl_session_cache shared:SSL:10m;
        ssl_session_timeout 1h;
        ssl_session_tickets off;

        # Add OCSP stapling if your cert supports it
        ssl_stapling on;
        ssl_stapling_verify on;

        # The SSL certificate we use is generated by https://github.com/kshcherban/acme-nginx
        # for the "*.blot.im" and "blot.im" domains. This allows us to do an infinite
        # number of subdomains for previewing templates. The paths to these files are
        # hardcoded in the source for kshcherban/acme-nginx. I wrote more about this
        # process in notes/wildcard-ssl
        ssl_certificate /etc/ssl/private/letsencrypt-domain.pem;
        ssl_certificate_key /etc/ssl/private/letsencrypt-domain.key;
        auth_basic "Protected";
        auth_basic_user_file /home/ec2-user/netdataconfig/netdata/passwords;

        # Redirect root to /v3 to avoid annoying login window
        location = / {
            return 301 $scheme://$host/v3/;
        }

        location / {
            proxy_pass http://localhost:19999;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header Authorization $http_authorization;

            # WebSocket support
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            
            # Timeouts
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
    }


    # blot.im
    server {
        listen 443 ssl;
        
        http2 on;

        server_name blot.im; 

        # Check with the SSL evaluator tool
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_prefer_server_ciphers off;
        ssl_ciphers "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384";

        # Hopefully improves performance
        # one megabyte can store about 4000 sessions
        # 10m = 40,000 sessions
        ssl_session_cache shared:SSL:10m;
        ssl_session_timeout 1h;
        ssl_session_tickets off;

        # Add OCSP stapling if your cert supports it
        ssl_stapling on;
        ssl_stapling_verify on;

        # The SSL certificate we use is generated by https://github.com/kshcherban/acme-nginx
        # for the "*.blot.im" and "blot.im" domains. This allows us to do an infinite
        # number of subdomains for previewing templates. The paths to these files are
        # hardcoded in the source for kshcherban/acme-nginx. I wrote more about this
        # process in notes/wildcard-ssl
        ssl_certificate /etc/ssl/private/letsencrypt-domain.pem;
        ssl_certificate_key /etc/ssl/private/letsencrypt-domain.key;
        set $upstream_server blot_node;

        # Match /sites/{USERNAME}/status where {USERNAME} is dynamic
        location ~ ^/sites/[^/]+/status$ {
          proxy_pass http://blot_node;
          proxy_read_timeout 24h;
          proxy_send_timeout 24h;
          proxy_connect_timeout 24h;

          # SSE-specific configurations
          proxy_http_version 1.1;      # SSE requires HTTP/1.1
          proxy_set_header Connection '';  # Disable keep-alive
          proxy_buffering off;         # Disable response buffering
          proxy_cache off;             # Ensure no caching

          # generic
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
        }

        location = /health {
          return 200;
        }

        #  redirect for /cdn/XYZ to cdn.blot.im/XYZ
        location ~ ^/cdn/(.*)$ {
          return 301 https://cdn.blot.im/$1;
        }

        # bypass cache
        location = /redis-health {
          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream
        }

        # bypass cache
        location /sites {
          client_max_body_size 100M;
          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream
        }

        # bypass cache
        location /dashboard {
          client_max_body_size 100M;
          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream
        }

        # bypass cache
        location /questions/ask {
          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream
        }

        location ~ ^/questions/[^/]+/edit$ {
          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream
        }

        location ~ ^/questions/[^/]+/new$ {
          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream
        }

        # git and icloud client need large bodies
        location /clients/git {
          client_max_body_size 1000M;
          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream
        }

        location /clients/icloud {
          client_max_body_size 100M;
          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream
        }

        location /clients {
          client_max_body_size 100M;
          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream
        }

        # error page for when node server is unavailable
        location = /error-upstream-offline.html {
          root  /home/ec2-user/openresty/html;
          try_files /error-upstream-offline-site.html =404;
        }

        location / {
          set $cache_key $scheme://$host$request_uri;

          open_file_cache off;

          proxy_cache            PROXY_CACHE;

          # https://cassiomolin.com/2016/09/09/which-http-status-codes-are-cacheable/
          proxy_cache_valid  200 301 302 400 404  1y;

          # we want to cache everything, even if cache-control says not to
          # it's important to ignore the vary header from upstream otherwise
          # nginx will cache multiple copies of the same page
          proxy_ignore_headers Cache-Control Expires Set-Cookie Vary;

          proxy_cache_key $cache_key;
          proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504;

          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream

          # This registers a cached response in our dictionary of cached responses
          # pass in the proxy_cache_key setting
          log_by_lua_block {
              if ngx.var.upstream_cache_status == "MISS" then
                  cacher:add(ngx.var.host, ngx.var.cache_key)
              end
          }        }

    }

    # webhooks relay at webhooks.blot.im
    server {
        listen 443 ssl;
        http2 on;
        server_name webhooks.blot.im; 

        # Dynamic handler for issuing or returning certs for SNI domains.
        ssl_certificate_by_lua_block {
          auto_ssl:ssl_certificate()
        }

        # Endpoint used for performing domain verification with Let's Encrypt.
        location /.well-known/acme-challenge/ {
          content_by_lua_block {
            auto_ssl:challenge_server()
          }
        }

        # You must still define a static ssl_certificate file for nginx to start.
        # The SSL certificate we use is generated by https://github.com/kshcherban/acme-nginx
        # for the "*.blot.im" and "blot.im" domains. This allows us to do an infinite
        # number of subdomains for previewing templates. The paths to these files are
        # hardcoded in the source for kshcherban/acme-nginx. I wrote more about this
        # process in notes/wildcard-ssl
        # Check with the SSL evaluator tool
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_prefer_server_ciphers off;
        ssl_ciphers "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384";

        # Hopefully improves performance
        # one megabyte can store about 4000 sessions
        # 10m = 40,000 sessions
        ssl_session_cache shared:SSL:10m;
        ssl_session_timeout 1h;
        ssl_session_tickets off;

        # Add OCSP stapling if your cert supports it
        ssl_stapling on;
        ssl_stapling_verify on;

        # The SSL certificate we use is generated by https://github.com/kshcherban/acme-nginx
        # for the "*.blot.im" and "blot.im" domains. This allows us to do an infinite
        # number of subdomains for previewing templates. The paths to these files are
        # hardcoded in the source for kshcherban/acme-nginx. I wrote more about this
        # process in notes/wildcard-ssl
        ssl_certificate /etc/ssl/private/letsencrypt-domain.pem;
        ssl_certificate_key /etc/ssl/private/letsencrypt-domain.key;
        location / {
            # the icloud relay needs to handle large file uploads
            client_max_body_size 100M;

            # IMPORTANT – we only send webhook relay requests to one
            # upstream (the master node - blot-container-green) because
            # it has an inmemory dictionary of connected clients – if
            # you have multiple upstreams, it doesn't work – 
            # DO NOT replace with http://blot_node
            proxy_pass http://127.0.0.1:8089;

            proxy_read_timeout 24h;
            proxy_send_timeout 24h;
            proxy_connect_timeout 24h;

            # SSE-specific configurations
            proxy_http_version 1.1;      # SSE requires HTTP/1.1
            proxy_set_header Connection '';  # Disable keep-alive
            proxy_buffering off;         # Disable response buffering
            proxy_cache off;             # Ensure no caching

            # generic
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header X-Request-ID $request_id;        
        }
    }

    # custom domains
    server {
        listen 80 default_server;
        listen 443 ssl default_server;
        http2 on;
        # Dynamic handler for issuing or returning certs for SNI domains.
        ssl_certificate_by_lua_block {
          auto_ssl:ssl_certificate()
        }

        # Endpoint used for performing domain verification with Let's Encrypt.
        location /.well-known/acme-challenge/ {
          content_by_lua_block {
            auto_ssl:challenge_server()
          }
        }

        # You must still define a static ssl_certificate file for nginx to start.
        # The SSL certificate we use is generated by https://github.com/kshcherban/acme-nginx
        # for the "*.blot.im" and "blot.im" domains. This allows us to do an infinite
        # number of subdomains for previewing templates. The paths to these files are
        # hardcoded in the source for kshcherban/acme-nginx. I wrote more about this
        # process in notes/wildcard-ssl
        # Check with the SSL evaluator tool
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_prefer_server_ciphers off;
        ssl_ciphers "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384";

        # Hopefully improves performance
        # one megabyte can store about 4000 sessions
        # 10m = 40,000 sessions
        ssl_session_cache shared:SSL:10m;
        ssl_session_timeout 1h;
        ssl_session_tickets off;

        # Add OCSP stapling if your cert supports it
        ssl_stapling on;
        ssl_stapling_verify on;

        # The SSL certificate we use is generated by https://github.com/kshcherban/acme-nginx
        # for the "*.blot.im" and "blot.im" domains. This allows us to do an infinite
        # number of subdomains for previewing templates. The paths to these files are
        # hardcoded in the source for kshcherban/acme-nginx. I wrote more about this
        # process in notes/wildcard-ssl
        ssl_certificate /etc/ssl/private/letsencrypt-domain.pem;
        ssl_certificate_key /etc/ssl/private/letsencrypt-domain.key;
        set $upstream_server blot_blogs_node;
        root /;

        # Match urls which start with /draft/stream/
        location /draft/stream/ {
          proxy_pass http://blot_node;
          proxy_read_timeout 24h;
          proxy_send_timeout 24h;
          proxy_connect_timeout 24h;

          # SSE-specific configurations
          proxy_http_version 1.1;      # SSE requires HTTP/1.1
          proxy_set_header Connection '';  # Disable keep-alive
          proxy_buffering off;         # Disable response buffering
          proxy_cache off;             # Ensure no caching

          # generic
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
        }

        # This is used to determine whether the server is handling
        # requests, don't remove it unless you change monit.rc too!
        location = /health {
          return 200;
        }

        # This is used to prevent people from accessing the git repositories in user folders
        location ^~ /.git {
          return 404;
        }

        # return 404 immediately for all requests to URLs ending in .php, .asp, .aspx, .jsp, .php5, .php7, .php8
        location ~* \.(?:php[0-9]?|asp|aspx|jsp)$ {
          return 404;
        }

        # return 404 immediate for /.vscode/sftp.json
        location = /.vscode/sftp.json {
          return 404;
        }

        location = /sftp-config.json {
          return 404;
        }

        location = /magento_version {
          return 404;
        }

        # return 404 immediately for all requests to URLs starting with /api/v1
        location ~* ^/api/v1 {
          return 404;
        }

        location ~* ^/api/collections/ {
          return 404;
        }

        location ~* ^/vendor/phpunit/ {
          return 404;
        }

        # return 404 for all URLS starting with /admin/controller/extension/extension
        location ~* ^/admin/controller/extension/extension {
          return 404;
        }

        # return 404 immediately for all requests to URLs starting with /admin/.git/
        location ~* ^/admin/.git {
          return 404;
        }

        # return 404 immediately for all requests containing 'index.php/' or 'admin.php/' anywhere in their path
        location ~* (?:index|admin)\.php/ {
          return 404;
        }

        # return 404 immediately for all requests to URLs ending in .env
        location ~* \.env$ {
          return 404;
        }

        # return 404 immediately for all requests to URLs containing '/wp-admin/', '/wp-content/', '/wp-includes/' anywhere in their path
        location ~* wp-(?:admin|content|includes|diambar|json|config) {
          return 404;
        }

        # bypass the cache for requests to /random
        # todo: handle this with headers instead of a separate location block
        location = /random {
          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream
        }

        # error page for when node server is unavailable
        location = /error-upstream-offline.html {
          root  /home/ec2-user/openresty/html;
          try_files /error-upstream-offline-blog.html =404;
        }

        location / {
          set $cache_key $scheme://$host$request_uri;

          open_file_cache off;

          proxy_cache            PROXY_CACHE;

          # https://cassiomolin.com/2016/09/09/which-http-status-codes-are-cacheable/
          proxy_cache_valid  200 301 302 400 404  1y;

          # we want to cache everything, even if cache-control says not to
          # it's important to ignore the vary header from upstream otherwise
          # nginx will cache multiple copies of the same page
          proxy_ignore_headers Cache-Control Expires Set-Cookie Vary;

          proxy_cache_key $cache_key;
          proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504;

          add_header 'Blot-Server' 'us' always;
          add_header 'Blot-Cache' $upstream_cache_status always;
          add_header 'Blot-Upstream' $upstream_addr always;

          proxy_pass http://$upstream_server;
          proxy_http_version 1.1;

          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Request-ID $request_id;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $http_connection;  # Use $http_connection for better dynamic handling
          # We let nginx handle the compression
          proxy_set_header Accept-Encoding "";

          gzip on;
          gzip_types text/plain text/css application/json application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript;
          gzip_comp_level 6;
          gzip_proxied any;

          # For larger responses (blog posts, content-heavy pages)
          # Configured for 2MB total buffer size
          proxy_buffers 32 64k;        # 2MB total (32 * 64k = 2048k)
          proxy_buffer_size 64k;

          proxy_redirect off;

          proxy_intercept_errors on;
          error_page 500 502 503 504 429 /error-upstream-offline.html;

          # Enable retrying the next upstream on failure
          # Important to limit the number of tries and possibly
          # the timeout otherwise a single request which triggers an error could take down all the servers
          # We do rolling deploys where one server is taken down at a time so this should allow 
          # a failover in that case. Expensive or bug-triggering requests should consistently failwhale
          # You have to be careful that these error codes are only returned when the server is truly
          # unavailable – I ran into a nasty bug where a request that should have lead to a 404 was return
          # a 500 error code, and this config considered the upstream server bad!
          proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
          proxy_next_upstream_tries 1;
          proxy_next_upstream_timeout 10s;

          # Fine-tune timeouts for better failover
          proxy_connect_timeout 3s;  # Time to establish a connection to the upstream
          proxy_send_timeout 10s;    # Time to send the request to the upstream
          proxy_read_timeout 15s;    # Time to read the response from the upstream

          # This registers a cached response in our dictionary of cached responses
          # pass in the proxy_cache_key setting
          log_by_lua_block {
              if ngx.var.upstream_cache_status == "MISS" then
                  cacher:add(ngx.var.host, ngx.var.cache_key)
              end
          }        }

    }    

    # redirect blot.im over HTTP to HTTPS
    server {
        listen 80;
        server_name blot.im; 
        return 301 https://$host$request_uri;
    }

    # internal server for inspecting and purging the cache
    server {
       listen 127.0.0.1:80;
        # needed by the node application running inside docker
        # which doesn't have access to the host network
        # this seems to cause errors for custom domains
        # and overrides the default server
        listen 172.30.0.122:8077;
            
        location = /inspect {
            content_by_lua_block {
                cacher:inspect(ngx)
            }
        }

        location = /rehydrate {
            content_by_lua_block {
                local message = cacher:rehydrate(ngx)
                ngx.say(message)
            }
        }

        location = /purge {
            content_by_lua_block {
                cacher:purge(ngx)
            }
        }

        # otherwise, return 404
        location / {
            return 404;
        }
    }
}